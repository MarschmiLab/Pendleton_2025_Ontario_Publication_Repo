---
title: "DADA2 Processing" 
author: "Augustus Pendleton"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: default
    keep_md: yes
    theme: journal
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---
<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
</style>


```{r setup, include=FALSE}
# For width of code chunks and scroll bar 
options(width=250)

knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE, 
                      include = TRUE,
                      warning = FALSE,
                      collapse = FALSE,
                      message = FALSE,
                      dpi=200, dev = "png",
                      engine = "R", # Chunks will always have R code, unless noted
                      error = TRUE,
                      fig.path="../figures/01_DADA2_Processing/",  
                      fig.align = "center") 

```

# Loading packages 
```{r load-packages}
# Efficient loading of the packages 
pacman::p_load(dada2, tidyverse, patchwork, phyloseq, Biostrings, fun.gus, install = FALSE)

knitr::write_bib(file = "data/01_dada2_exports/packages.bib")

# Load in the colors and shapes
source("code/R/plotting_aesthetics.R")
```

# Goals of this document

In this document, I read in raw reads from run "231220_M05470_0458_000000000-L9NR8", Order #10471351, where I sequencing the V4 region of the 16S gene for whole-fraction samples from Lake Ontario. I clean-up metadata, examine read quality, filter, trim, remove chimeras, and assess how many reads per sample we retain. I also analyze accuracy in regards to the Mock Community.

To note, this entire process is run within the AAH_ONT_2023 home directory, which in the future will contain multiple projects that resulted from this cruise.

# Loading Metadata

In this section, I load data sheets from the lab data log, join them together, and calculate some stats including how long it took for us to filter and how much water was extracted for each sample.

```{r load-metadata-one}
full_metadata <- read_csv("data/01_dada2_exports/ontario_metadata.csv")
```

# Set the path to the seq files 

This assumes that you've downloaded the raw fastq.gz files from PRJNA1212049 into the directory data/00_raw_reads

```{r set-path}
# Set path to the gzipped files 
path <- "data/00_raw_reads"
path
# What files do we have?
list.files(path)
```

We have 151 samples, with forward and reverse reads for each. These include smaples included in our DNA_Log, as well as PCR_Blanks (from first-round PCR), "Blank1" and "Blank2" which are blanks from the indexing step, and "Zymo_Mock" which is our Zymo Microbiomics Mock Community.

# Load in Forward and Reverse reads and assess the quality

```{r read-and-quality}
# Create variable for the forward and the reverse reads

# 1. Forward read variable 
forward_reads <- sort(list.files(path, pattern = "R1.fastq.gz", 
                      full.names = TRUE))
forward_reads

# 2. Reverse read variable 
reverse_reads <- sort(list.files(path, pattern = "R2.fastq.gz", 
                      full.names = TRUE))
reverse_reads

# Get clean names
sample_names <- data.frame(bname = basename(forward_reads),
            rname = basename(reverse_reads)) %>% # Extract the basename (file name)
  separate_wider_delim(bname, delim = "_", names_sep = "_", too_few = "align_start") %>% # Split up by the underscores
  transmute(sample_name = ifelse(bname_1 == "Plate1",
                              paste(bname_3, bname_4, sep = "_"),
                              paste(bname_4, bname_5, sep = "_")))%>% # Plate 1 and Plate 2 had a diff. number of underscores until the DNA idea - hence the ifelse
  pull()

# 3. Prep filepaths for filtered files.
# Create a variable holding file names for the Forward and Reverse filtered reads 

filtered_forward_reads <- file.path("data", "filtered", paste0(sample_names, "_R1_filtered.fastq.gz"))
filtered_reverse_reads <- file.path("data", "filtered", paste0(sample_names, "_R2_filtered.fastq.gz"))

# An small view into the quality of some samples forward and reverse
set.seed(031491)
random_plots <- sample(1:151, size = 20)

plotQualityProfile(forward_reads[random_plots])
plotQualityProfile(reverse_reads[random_plots])

```

forward reads: 
- Pretty clean, in general
- Slight lower quality at beginning - maybe 15 bases in it gets pretty clean?
- The only samples that have a significant drop in quality halfway in are the Blanks/Negative Controls and AP_D33.
- The Mock community was also lower quality than all of our samples (annoying and a little strange)
- Read counts look good

reverse reads: 
- Worse quality overall than forward (expected)
- Most have some reads that start to drop ~220

aggregate plot:
- Overall pretty happy. 
- Forward and reverse get up to high quality within 20 bp
- Tbh it doesn't look like forward need to be right-trimmed, but will do a bit
- Reverse gonna right-trim to 220bp

# Filter and Trim
```{r filter-trim}
set.seed(031491)
# Major filtering step!
filtered_out <- filterAndTrim(forward_reads, filtered_forward_reads,
                              reverse_reads, filtered_reverse_reads,
                              truncLen = c(240,220), trimLeft = c(20,20),
                              maxN = 0, maxEE = c(1,1), truncQ = 2, 
                              rm.phix = TRUE, compress = TRUE, 
                              multithread = 10,
                              orient.fwd = "GTG")

# Plot the quality of trimmed reads! 
plotQualityProfile(filtered_forward_reads[random_plots])

plotQualityProfile(filtered_reverse_reads[random_plots])


#lines stay above 30 on all plots, median reads.in around 60k
filter_df <- as.data.frame(filtered_out)
cat("Median ",median(filter_df$reads.in), " reads in")
cat("Median ", median(filter_df$reads.out), "reads out")
cat("Median", median(filter_df$reads.in) - median(filter_df$reads.out), " reads removed")
```

Overall, dropped about 20,000 reads from each sample.

# Generate an error model 
```{r learn-errors}
# Learn errors
err_forward_reads <- learnErrors(filtered_forward_reads, multithread = TRUE) 


err_reverse_reads <- learnErrors(filtered_reverse_reads, multithread = TRUE)


# Plot the errors
# the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected
plotErrors(err_forward_reads, nominalQ = TRUE) 
plotErrors(err_reverse_reads, nominalQ = TRUE)
```

Predicted error hews fairly close to observed error, which is good.

# Inferring ASVs on the forward and reverse sequences 

```{r infer-ASVs}
# run dada2 on the forward sequences
dada_forward <- dada(filtered_forward_reads, err = err_forward_reads, multithread = TRUE)

# run dada2 on the reverse sequences 
dada_reverse <- dada(filtered_reverse_reads, err = err_reverse_reads, multithread = TRUE)
```


# Merge forward and reverse ASVs 
```{r merge-FandR-ASVs}
# Merge the forward ASVs and the reverse ASVs
merged_amplicons <- mergePairs(dada_forward, filtered_forward_reads, 
                               dada_reverse, filtered_reverse_reads,
                               verbose = TRUE)
# Evaluate the output 
length(merged_amplicons) #151
names(merged_amplicons)
```


# Generate a count table! 

```{r gen-countTable-seqTab}
seqtab <- makeSequenceTable(merged_amplicons)

dim(seqtab) # 149 20329

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# all sequences fall between 220 and 409
data.frame(Seq_Length = nchar(getSequences(seqtab))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram()


# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(seqtab))) %>%sort()
```

Okay so it looks like a majority of ASVs are length 252 and 253


# Check & Remove for Chimeras (Bimeras)

Check size after bimera removal - trimming step
Add table() command

Also look at tweaking trimming to see if that cleans up the Mock

```{r check-chimeras}
# Identify and remove chimeras 
seqtab_nochim <- removeBimeraDenovo(seqtab, verbose = TRUE)
# Identified 1663 bimeras out of 21502 input sequences.


data.frame(Seq_Length_NoChim = nchar(getSequences(seqtab_nochim))) %>%
  ggplot(aes(x = Seq_Length_NoChim )) + 
  geom_histogram()


asvs <- dim(seqtab_nochim)[2]

# What proportion of counts were removed? 
chim_check <- sum(seqtab_nochim)/sum(seqtab) #  0.9835776
frac_removed <- (1-chim_check)*100
frac_removed # 1.642241
```

Chimeras represented `r frac_removed` percent of the data, with parameters trimLeft(20,20).

Dataset includes `r asvs` ASVs.

# Size Selection

I also want to get rid of ASVs that too big.

```{r size selection}
asv_keeps <- nchar(getSequences(seqtab_nochim)) %in% c(252,253) # Find which sequences have length 252 or 253

seqtab_nc_len <- seqtab_nochim[,asv_keeps] # Remove other sequences

data.frame(Seq_Length_NoChim_Len = nchar(getSequences(seqtab_nc_len))) %>%
  ggplot(aes(x = Seq_Length_NoChim_Len )) + 
  geom_histogram()
```

We removed all ASVs that weren't size 252 or 253.

# Track the sequences through the pipeline 

```{r track-seqs}
# create a little function to identify number seqs 
getN <- function(x) sum(getUniques(x))

# Make the table to track the seqs 
track <- cbind(filtered_out, 
               sapply(dada_forward, getN),
               sapply(dada_reverse, getN),
               sapply(merged_amplicons, getN),
               rowSums(seqtab_nc_len))

head(track)

# Change column names 
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- sample_names

# Generate a plot to track the reads through our DADA2 pipeline
track %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "sample_name") %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  #left_join(metadata, by = "sample_name") %>% 
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim"),
         is_blank = sample_name%in%c("PCR_Blanks","AP_D61","AP_D62","AP_D153","AP_D154")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type, color = is_blank)) + 
  #facet_grid(~strata) + 
  geom_line(aes(group = sample_name)) + 
  geom_point(shape = 21, size = 3, alpha = 0.8, color = "grey") + 
  scale_fill_brewer(palette = "Spectral") + 
  theme_bw() + 
  labs(x = "Filtering Step", y = "Number of Sequences") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

# add metadata
# make track dataframe and edit names to match metadata
track_df <- as.data.frame(track) %>% 
  rownames_to_column(var = "DNA_ID") %>%
  mutate(perc_reads_retained = 100 * nochim / input)

#merge
meta_track <- left_join(track_df, full_metadata, by = "DNA_ID")

#DNA Concentrations graph
dna_nochim <- meta_track %>% 
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  filter(read_type == "nochim")

dna_nochim %>% 
  ggplot(aes(x = Concentration_ng_ul, y = num_reads)) + 
  geom_point() + 
  facet_wrap(~Depth_Class)+
  theme_classic() + 
  labs(x = "DNA Concentration (ng/uL)", y = "Number of reads") +
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

dna_nochim %>% 
  ggplot(aes(x = Concentration_ng_ul, y = perc_reads_retained)) + 
  geom_point() + 
  facet_wrap(~Depth_Class)+
  theme_classic() + 
  labs(x = "DNA Concentration (ng/uL)", y = "Percent reads retained") +
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
  
```

Read retention looks good, and more importantly, consistent between depths and across original DNA concentrations. There is some drop in read counts as DNA concentrations increase. My hypothesis is that those are very high biomass (brown) samples whose DNA extractions may have been a little dirtier, and so in the end produced slighlty worse read quality. Or, higher sample diversity within them could lead to worse read quality/ability to distinguish ASVs.


# Prepare the data for export! 
## 1. ASV Table 

```{r prepare-ASV-table}
# Prep the asv table! 

samples_out <- rownames(seqtab_nc_len)

# Pull out sample names from the fastq file name 

sample_names_reformatted <- str_remove(samples_out, "_R1_filtered.fastq.gz")

# Replace the names in our seqtable 
rownames(seqtab_nc_len) <- sample_names_reformatted

### intuition check 
stopifnot(rownames(seqtab_nc_len) == sample_names_reformatted)

############## Modify the ASV names and then save a fasta file! 
# Give headers more manageable names
# First pull the ASV sequences
asv_seqs <- colnames(seqtab_nc_len)

# make headers for our ASV seq fasta file, which will be our asv names
asv_headers <- vector(dim(seqtab_nc_len)[2], mode = "character")

# loop through vector and fill it in with ASV names 

for (i in 1:dim(seqtab_nc_len)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

# intitution check
asv_headers


##### Rename ASVs in table then write out our ASV fasta file! 
#View(seqtab_nochim)
asv_tab <- t(seqtab_nc_len)
#View(asv_tab)

## Rename our asvs! 
row.names(asv_tab) <- sub(">", "", asv_headers)
#View(asv_tab)

# Write the count table to a file! 
# You may need to create the 01_dada2_exports folder
write.table(asv_tab, "data/01_dada2_exports/ASV_counts.tsv", sep = "\t", quote = FALSE, col.names = NA)

# Write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))

# Save to a file!
write(asv_fasta, "data/01_dada2_exports/ASVs.fasta")

# Also write to TaxAss folder
# You may need to make the TaxAss_analysis folder
write(asv_fasta, "data/TaxAss_analysis/ASVs.fasta")

# And save our seqtab_nc_len R Object for taxonomic assignment
save(seqtab_nc_len, file = "data/01_dada2_exports/seqtab_nc_len.RData")

# And save our meta_track R object
save(meta_track, file = "data/01_dada2_exports/meta_track.RData")

```


# Errata (not included in final outputs)

## Testing more stringent trimming - don't do!

```{r more-stringent-trimming, eval = FALSE}

filtered_forward_reads_ag <- file.path("data", "filtered_aggressive", paste0(sample_names, "_R1_filtered.fastq.gz"))
filtered_reverse_reads_ag <- file.path("data", "filtered_aggressive", paste0(sample_names, "_R2_filtered.fastq.gz"))


filtered_out_ag <- filterAndTrim(forward_reads, filtered_forward_reads_ag,
                              reverse_reads, filtered_reverse_reads_ag,
                              truncLen = c(220,200), trimLeft = c(25,25),
                              maxN = 0, maxEE = c(1,1), truncQ = 2, 
                              rm.phix = TRUE, compress = TRUE, 
                              multithread = TRUE)

err_forward_reads_ag <- learnErrors(filtered_forward_reads_ag, multithread = TRUE) 

err_reverse_reads_ag <- learnErrors(filtered_reverse_reads_ag, multithread = TRUE)

dada_forward_ag <- dada(filtered_forward_reads_ag, err = err_forward_reads_ag, multithread = TRUE)

# run dada2 on the reverse sequences 
dada_reverse_ag <- dada(filtered_reverse_reads_ag, err = err_reverse_reads_ag, multithread = TRUE)


merged_amplicons_ag <- mergePairs(dada_forward_ag, filtered_forward_reads_ag, 
                               dada_reverse_ag, filtered_reverse_reads_ag,
                               verbose = TRUE)

seqtab_ag <- makeSequenceTable(merged_amplicons_ag)

dim(seqtab_ag) # 151 21008

data.frame(Seq_Length_ag = nchar(getSequences(seqtab_ag))) %>%
  ggplot(aes(x = Seq_Length_ag )) + 
  geom_histogram()

table(nchar(getSequences(seqtab_ag))) %>% sort()

seqtab_nochim_ag <- removeBimeraDenovo(seqtab_ag, verbose = TRUE) 
# Identified 1167 bimeras out of 21008 input sequences.

asv_keeps_ag <- nchar(getSequences(seqtab_nochim_ag)) %in% c(242, 243)


seqtab_nc_len_ag <- seqtab_nochim_ag[,asv_keeps_ag]

track_ag <- cbind(filtered_out_ag, 
               sapply(dada_forward_ag, getN),
               sapply(dada_reverse_ag, getN),
               sapply(merged_amplicons_ag, getN),
               rowSums(seqtab_nc_len_ag))


colnames(track_ag) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track_ag) <- sample_names


track_ag %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "sample_name") %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  #left_join(metadata, by = "sample_name") %>% 
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim"),
         is_blank = sample_name%in%c("PCR_Blanks","AP_D61","AP_D62","AP_D153","AP_D154")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type, color = is_blank)) + 
  #facet_grid(~strata) + 
  geom_line(aes(group = sample_name)) + 
  geom_point(shape = 21, size = 3, alpha = 0.8, color = "grey") + 
  scale_fill_brewer(palette = "Spectral") + 
  theme_bw() + 
  labs(x = "Filtering Step", y = "Number of Sequences") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))


# Check the mock commmunity 
mock_sample_ag <- seqtab_nc_len_ag["Zymo_Mock_R1_filtered.fastq.gz",]

length(mock_sample_ag)

# Drop ASVs absent from mock community 
length(mock_sample_ag[mock_sample_ag > 0])

mock_sample_sub_ag <- sort(mock_sample_ag[mock_sample_ag > 0], decreasing = TRUE)

length(mock_sample_sub_ag)

cat("DADA2 inferred", length(mock_sample_sub_ag), "ASVs present in the Mock Community.")

#Who are they in the mock community? 

#### Compare our ASVs from the mock community to the reference fasta!
mock_reference_ag <- getSequences(file.path("data/mock_fastas/", "mock_amplicons.fasta"))

matches_ag <- sapply(names(mock_sample_sub),
                             function(x) any(grepl(x, mock_reference)))

match_mock_ref_ag <- sum(matches_ag)

total_mock_reads_ag <- sum(mock_sample_sub_ag)
matched_reads_ag <- sum(mock_sample_sub_ag[matches_ag])
err_reads_ag <- sum(mock_sample_sub_ag[!matches_ag])


cat(sum(match_mock_ref_ag), "ASVs were exact matches to the expected reference sequences.")

cat("DADA2 inferred ", length(mock_sample_sub_ag) - match_mock_ref_ag, "erroneous ASVs out of the mock community with aggressive filtering")

cat("Erroneous matches represented ", 100*err_reads_ag/total_mock_reads_ag, "% of total reads in the mock with aggressive filtering")
```


# Session Info

```{r}
sessioninfo::session_info()
```